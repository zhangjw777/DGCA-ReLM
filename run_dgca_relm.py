"""
DGCA-ReLMè®­ç»ƒè„šæœ¬
æ”¯æŒDDPåŒå¡è®­ç»ƒï¼ŒåŸºäºåŸReLMæ”¹é€ 
"""

from __future__ import absolute_import, division, print_function

import argparse
import logging
import os
import random
import math

# ç¦ç”¨ tokenizers çš„å¹¶è¡Œï¼Œé¿å…ä¸ DataLoader å¤šè¿›ç¨‹å†²çª
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import numpy as np
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
from torch.utils.data.distributed import DistributedSampler
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from transformers import AutoTokenizer, BertForMaskedLM
from transformers import SchedulerType, get_scheduler

# é¡¹ç›®æ¨¡å—
from utils.data_processor import EcspellProcessor
from utils.metrics import Metrics
from utils.dgca_data_processor import (
    convert_examples_to_dgca_features,
    create_dgca_dataset
)
from config.dgca_config import DGCAConfig
from confusion.confusion_utils import ConfusionSet
from multiTask.DGCAModel import DGCAReLMWrapper


logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO
)
logger = logging.getLogger(__name__)


def setup_ddp(args):
    """åˆå§‹åŒ–DDPç¯å¢ƒ"""
    # ä»ç¯å¢ƒå˜é‡è·å– local_rankï¼ˆtorchrun ä¼šè®¾ç½®è¿™ä¸ªç¯å¢ƒå˜é‡ï¼‰
    if 'LOCAL_RANK' in os.environ:
        args.local_rank = int(os.environ['LOCAL_RANK'])
    
    if args.local_rank != -1:
        torch.cuda.set_device(args.local_rank)
        device = torch.device("cuda", args.local_rank)
        dist.init_process_group(backend="nccl")
    else:
        device = torch.device("cuda" if torch.cuda.is_available() and not args.no_cuda else "cpu")
    
    return device


def cleanup_ddp():
    """æ¸…ç†DDPç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


def is_main_process(args):
    """åˆ¤æ–­æ˜¯å¦æ˜¯ä¸»è¿›ç¨‹ï¼ˆç”¨äºæ—¥å¿—è¾“å‡ºï¼‰"""
    return args.local_rank == -1 or args.local_rank == 0


def set_seed(seed, n_gpu):
    """è®¾ç½®éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if n_gpu > 0:
        torch.cuda.manual_seed_all(seed)


def dynamic_mask_token(inputs, targets, tokenizer, device, mask_mode="noerror", noise_probability=0.2):
    """
    ReLMçš„masked-FTæŠ€æœ¯ï¼šåœ¨æºå¥ä¸­éšæœºé®è”½ä¸€éƒ¨åˆ†éé”™è¯¯å­—ç¬¦
    ä¼˜åŒ–ç‰ˆï¼šå…¨éƒ¨åœ¨GPUä¸Šæ‰§è¡Œï¼Œé¿å…CPU-GPUæ•°æ®ä¼ è¾“
    """
    inputs = inputs.clone()
    
    # ç›´æ¥åœ¨GPUä¸Šåˆ›å»ºæ¦‚ç‡çŸ©é˜µ
    probability_matrix = torch.full(inputs.shape, noise_probability, device=device)
    
    # æ„å»ºspecial tokens maskï¼ˆåœ¨GPUä¸Šï¼‰
    # ä½¿ç”¨å‘é‡åŒ–æ“ä½œæ›¿ä»£åˆ—è¡¨æ¨å¯¼
    pad_id = tokenizer.pad_token_id
    cls_id = tokenizer.cls_token_id
    sep_id = tokenizer.sep_token_id
    mask_id = tokenizer.mask_token_id
    
    special_tokens_mask = (
        (inputs == pad_id) | 
        (inputs == cls_id) | 
        (inputs == sep_id) |
        (inputs == mask_id)
    )
    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)
    
    if mask_mode == "noerror":
        probability_matrix.masked_fill_(inputs != targets, value=0.0)
    elif mask_mode == "error":
        probability_matrix.masked_fill_(inputs == targets, value=0.0)
    # else: mask_mode == "all", keep as is
    
    masked_indices = torch.bernoulli(probability_matrix).bool()
    inputs[masked_indices] = mask_id
    
    return inputs


def main():
    parser = argparse.ArgumentParser()
    
    # ============ æ•°æ®é…ç½® ============
    parser.add_argument("--data_dir", type=str, default="data/ecspell/",
                        help="æ•°æ®ç›®å½•")
    parser.add_argument("--load_model_path", type=str, default="bert-base-chinese",
                        help="é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„")
    parser.add_argument("--cache_dir", type=str, default="../../cache/",
                        help="æ¨¡å‹ç¼“å­˜ç›®å½•")
    parser.add_argument("--output_dir", type=str, default="outputs/dgca_relm/",
                        help="è¾“å‡ºç›®å½•")
    parser.add_argument("--load_state_dict", type=str, default="",
                        help="åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹æƒé‡")
    
    # ============ DGCAé…ç½® ============
    parser.add_argument("--dgca_config", type=str, default="config/default_config.yaml",
                        help="DGCAé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆåŒ…å«æ··æ·†é›†è·¯å¾„ã€å€™é€‰é›†å¤§å°ç­‰ï¼‰")
    parser.add_argument("--ablation", type=str, default=None,
                        help="ä½¿ç”¨é¢„å®šä¹‰çš„æ¶ˆèé…ç½®ï¼šfull/detector_only/candidate_only/baseline")
    
    # ============ è®­ç»ƒé…ç½® ============
    parser.add_argument("--do_train", action="store_true", help="æ˜¯å¦è®­ç»ƒ")
    parser.add_argument("--do_eval", action="store_true", help="æ˜¯å¦éªŒè¯")
    parser.add_argument("--do_test", action="store_true", help="æ˜¯å¦æµ‹è¯•")
    parser.add_argument("--train_on", type=str, default="law", help="è®­ç»ƒé›†åˆ’åˆ†ï¼šlaw/med/odw")
    parser.add_argument("--eval_on", type=str, default="law")
    parser.add_argument("--test_on", type=str, default="law")
    
    # ============ é¢„å¤„ç†æ•°æ®ï¼ˆå¯é€‰ï¼ŒåŠ é€Ÿè®­ç»ƒï¼‰ ============
    parser.add_argument("--preprocessed_train", type=str, default=None)
    parser.add_argument("--preprocessed_eval", type=str, default=None)
    parser.add_argument("--preprocessed_test", type=str, default=None)
    
    # ============ æ¨¡å‹ä¸æ•°æ®è¶…å‚ ============
    parser.add_argument("--prompt_length", type=int, default=3)
    parser.add_argument("--max_seq_length", type=int, default=128)
    parser.add_argument("--do_lower_case", action="store_true")
    parser.add_argument("--use_slow_tokenizer", action="store_true")
    
    # ============ è®­ç»ƒè¶…å‚ ============
    parser.add_argument("--train_batch_size", type=int, default=64)
    parser.add_argument("--eval_batch_size", type=int, default=64)
    parser.add_argument("--learning_rate", type=float, default=5e-5)
    parser.add_argument("--num_train_epochs", type=float, default=10.0)
    parser.add_argument("--max_train_steps", type=int, default=None)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1)
    parser.add_argument("--warmup_proportion", type=float, default=0.1)
    parser.add_argument("--weight_decay", type=float, default=0.01)
    parser.add_argument("--lr_scheduler_type", type=SchedulerType, default="linear")
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--save_steps", type=int, default=5000,
                        help="ä¿å­˜æ¨¡å‹çš„é¢‘ç‡ï¼ˆé»˜è®¤5000æ­¥ï¼‰")
    parser.add_argument("--eval_steps", type=int, default=None,
                        help="è¯„ä¼°é¢‘ç‡ï¼ˆé»˜è®¤=save_stepsï¼Œå¤§æ•°æ®é›†å»ºè®®è®¾ç½®æ›´å¤§å€¼å¦‚10000-50000ï¼‰")
    parser.add_argument("--logging_steps", type=int, default=1000)
    
    # ============ Early Stopping ============
    parser.add_argument("--early_stopping_patience", type=int, default=None,
                        help="Early stopping patienceï¼Œè¿ç»­ N æ¬¡ eval æŒ‡æ ‡ä¸æå‡åˆ™åœæ­¢ã€‚"
                             "é»˜è®¤Noneè¡¨ç¤ºä¸å¯ç”¨")
    parser.add_argument("--early_stopping_metric", type=str, default="f1",
                        choices=["f1", "f2", "precision", "recall"],
                        help="Early stopping ç›‘æ§çš„æŒ‡æ ‡")
    
    # ============ ç¡¬ä»¶é…ç½® ============
    parser.add_argument("--no_cuda", action="store_true")
    parser.add_argument("--fp16", action="store_true", help="æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦")
    parser.add_argument("--local_rank", type=int, default=-1, help="DDP local rank")
    parser.add_argument("--num_workers", type=int, default=4, help="DataLoaderçš„å·¥ä½œè¿›ç¨‹æ•°")
    parser.add_argument("--preload_data", action="store_true", 
                        help="é¢„åŠ è½½æ•°æ®åˆ°å†…å­˜ï¼ˆéœ€è¦è¶³å¤ŸRAMï¼Œä½†å¯æ˜¾è‘—æå‡é€Ÿåº¦ï¼‰")
    parser.add_argument("--prefetch_factor", type=int, default=2,
                        help="DataLoaderé¢„å–å› å­ï¼Œæ¯ä¸ªworkeré¢„å–çš„batchæ•°é‡")
    
    # ============ DDPä¼˜åŒ–å‚æ•° ============
    parser.add_argument("--ddp_bucket_cap_mb", type=int, default=100,
                        help="DDP bucketå¤§å°(MB)ï¼Œä½å¸¦å®½ç¯å¢ƒå»ºè®®è®¾ç½®æ›´å¤§å€¼å¦‚200")
    parser.add_argument("--ddp_static_graph", action="store_true",
                        help="å¯ç”¨static_graphä¼˜åŒ–ï¼ˆé€‚åˆå›ºå®šè®¡ç®—å›¾çš„æ¨¡å‹ï¼‰")
    
    # ============ ReLMç›¸å…³ ============
    parser.add_argument("--mft", action="store_true", help="masked-fine-tuning")
    parser.add_argument("--mask_mode", type=str, default="noerror", 
                        help="é®è”½æ¨¡å¼ï¼šnoerror/error/all")
    parser.add_argument("--mask_rate", type=float, default=0.2)
    parser.add_argument("--anchor", type=str, default=None, help="é”šç‚¹å­—ç¬¦ä¸²")
    parser.add_argument("--apply_prompt", action="store_true", help="ä½¿ç”¨prompt")
    parser.add_argument("--freeze_lm", action="store_true", help="å†»ç»“è¯­è¨€æ¨¡å‹")
    
    args = parser.parse_args()
    
    # ============ åˆå§‹åŒ–ç¯å¢ƒ ============
    device = setup_ddp(args)
    
    n_gpu = 1 if args.local_rank != -1 else torch.cuda.device_count()
    
    if is_main_process(args):
        logger.info(f"Device: {device}, n_gpu: {n_gpu}, DDP: {args.local_rank != -1}, FP16: {args.fp16}")
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed, n_gpu)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if is_main_process(args) and not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)
    
    # ============ åŠ è½½DGCAé…ç½® ============
    if args.ablation:
        from config.dgca_config import get_ablation_config
        dgca_config = get_ablation_config(args.ablation)
        if is_main_process(args):
            logger.info(f"Using ablation config: {args.ablation}")
    else:
        dgca_config = DGCAConfig.from_yaml(args.dgca_config)
    
    if is_main_process(args):
        logger.info(f"DGCA Config:\n{dgca_config}")
        # ä¿å­˜é…ç½®
        dgca_config.to_yaml(os.path.join(args.output_dir, "dgca_config.yaml"))
    
    # ============ åŠ è½½Tokenizer ============
    tokenizer = AutoTokenizer.from_pretrained(
        args.load_model_path,
        do_lower_case=args.do_lower_case,
        cache_dir=args.cache_dir,
        use_fast=not args.use_slow_tokenizer
    )
    
    # ============ åŠ è½½æ··æ·†é›† ============
    confusion_set = ConfusionSet(
        confusion_dir=dgca_config.confusion_dir,
        confusion_file=dgca_config.confusion_file,
        tokenizer=tokenizer,
        candidate_size=dgca_config.candidate_size,
        include_original=dgca_config.include_original_char
    )
    
    if is_main_process(args):
        logger.info(f"Loaded confusion set with {len(confusion_set.confusion_dict)} entries")
    
    # ============ å¤„ç†anchor ============
    anchor = None
    if args.anchor is not None:
        anchor = [tokenizer.sep_token] + [t for t in args.anchor]
    
    # ============ æ•°æ®å¤„ç† ============
    processor = EcspellProcessor()
    
    # å¯¼å…¥é¢„å¤„ç†æ•°æ®é›†ç±»
    from utils.dgca_data_processor import PreprocessedDataset
    
    if args.do_train:
        # ä¼˜å…ˆä½¿ç”¨é¢„å¤„ç†æ•°æ®
        if args.preprocessed_train:
            if is_main_process(args):
                logger.info(f"Loading preprocessed training data from {args.preprocessed_train}")
                if args.preload_data:
                    logger.info("Preloading data to memory (this may take a while for large datasets)...")
            
            # DDPæ¨¡å¼ä¸‹ï¼Œè®©rank 0å…ˆåŠ è½½ï¼Œå…¶ä»–rankç­‰å¾…ï¼Œé¿å…I/Oç«äº‰
            if args.local_rank != -1:
                if args.local_rank == 0:
                    train_dataset = PreprocessedDataset(
                        args.preprocessed_train, 
                        preload_to_memory=args.preload_data
                    )
                dist.barrier()  # rank 0 åŠ è½½å®Œæˆåï¼Œå…¶ä»–rankå†å¼€å§‹
                if args.local_rank != 0:
                    train_dataset = PreprocessedDataset(
                        args.preprocessed_train,
                        preload_to_memory=args.preload_data
                    )
            else:
                train_dataset = PreprocessedDataset(
                    args.preprocessed_train,
                    preload_to_memory=args.preload_data
                )
        else:
            # åŸå§‹æ–¹å¼ï¼šè¯»å–txtæ–‡ä»¶å¹¶å¤„ç†
            train_examples = processor.get_train_examples(args.data_dir, args.train_on)
            train_features = convert_examples_to_dgca_features(
                train_examples,
                args.max_seq_length,
                tokenizer,
                confusion_set,
                args.prompt_length,
                anchor=anchor,
                mask_rate=args.mask_rate
            )
            train_dataset = create_dgca_dataset(train_features)
            if is_main_process(args):
                logger.info(f"Loaded {len(train_examples)} training examples")
        
        if is_main_process(args):
            logger.info(f"Training dataset size: {len(train_dataset)}")
        
        # DDP Sampler
        if args.local_rank != -1:
            train_sampler = DistributedSampler(train_dataset, shuffle=True)
        else:
            train_sampler = RandomSampler(train_dataset)
        
        # è®¡ç®—å®é™…batch size
        per_device_batch_size = args.train_batch_size // args.gradient_accumulation_steps
        
        # DataLoaderä¼˜åŒ–é…ç½®
        dataloader_kwargs = {
            'sampler': train_sampler,
            'batch_size': per_device_batch_size,
            'num_workers': args.num_workers,
            'pin_memory': True,
            'persistent_workers': args.num_workers > 0,
            'drop_last': True,  # é¿å…æœ€åä¸€ä¸ªbatchå¤§å°ä¸ä¸€è‡´å¯¼è‡´çš„é—®é¢˜
        }
        
        # æ·»åŠ prefetch_factorï¼ˆéœ€è¦num_workers > 0ï¼‰
        if args.num_workers > 0:
            dataloader_kwargs['prefetch_factor'] = args.prefetch_factor
        
        train_dataloader = DataLoader(train_dataset, **dataloader_kwargs)
        
        num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
        if args.max_train_steps is None:
            args.max_train_steps = int(args.num_train_epochs * num_update_steps_per_epoch)
        else:
            args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)
    
    if args.do_eval:
        # ä¼˜å…ˆä½¿ç”¨é¢„å¤„ç†æ•°æ®
        if args.preprocessed_eval:
            if is_main_process(args):
                logger.info(f"Loading preprocessed eval data from {args.preprocessed_eval}")
            
            # DDPæ¨¡å¼ä¸‹ï¼Œè®©rank 0å…ˆåŠ è½½ï¼Œå…¶ä»–rankç­‰å¾…ï¼Œé¿å…I/Oç«äº‰
            if args.local_rank != -1:
                if args.local_rank == 0:
                    eval_dataset = PreprocessedDataset(args.preprocessed_eval)
                dist.barrier()
                if args.local_rank != 0:
                    eval_dataset = PreprocessedDataset(args.preprocessed_eval)
            else:
                eval_dataset = PreprocessedDataset(args.preprocessed_eval)
        else:
            eval_examples = processor.get_dev_examples(args.data_dir, args.eval_on)
            eval_features = convert_examples_to_dgca_features(
                eval_examples,
                args.max_seq_length,
                tokenizer,
                confusion_set,
                args.prompt_length,
                anchor=anchor
            )
            eval_dataset = create_dgca_dataset(eval_features)
        
        eval_sampler = SequentialSampler(eval_dataset)
        eval_dataloader = DataLoader(
            eval_dataset,
            sampler=eval_sampler,
            batch_size=args.eval_batch_size,
            num_workers=args.num_workers,
            pin_memory=True
        )
    
    # ============ åˆ›å»ºæ¨¡å‹ ============
    bert_model = BertForMaskedLM.from_pretrained(
        args.load_model_path,
        return_dict=True,
        cache_dir=args.cache_dir
    )
    
    model = DGCAReLMWrapper(
        bert_model=bert_model,
        confusion_set=confusion_set,
        config=dgca_config,
        prompt_length=args.prompt_length
    )
    
    model.to(device)
    
    # åŠ è½½å·²æœ‰æƒé‡
    if args.load_state_dict:
        model.load_state_dict(torch.load(args.load_state_dict, map_location=device))
        if is_main_process(args):
            logger.info(f"Loaded model from {args.load_state_dict}")
    
    # DDPåŒ…è£…ï¼ˆä¼˜åŒ–é…ç½®ï¼‰
    if args.local_rank != -1:
        ddp_kwargs = {
            'device_ids': [args.local_rank],
            'output_device': args.local_rank,
            'bucket_cap_mb': args.ddp_bucket_cap_mb,
            'gradient_as_bucket_view': True,  # å‡å°‘å†…å­˜æ‹·è´
        }
        if args.ddp_static_graph:
            ddp_kwargs['static_graph'] = True
        
        model = DDP(model, **ddp_kwargs)
        
        if is_main_process(args):
            logger.info(f"DDP config: bucket_cap_mb={args.ddp_bucket_cap_mb}, "
                       f"gradient_as_bucket_view=True, static_graph={args.ddp_static_graph}")
    
    # ============ ä¼˜åŒ–å™¨ ============
    if args.do_train:
        no_decay = ["bias", "LayerNorm.bias", "LayerNorm.weight"]
        optimizer_grouped_parameters = [
            {
                "params": [p for n, p in model.named_parameters() 
                          if not any(nd in n for nd in no_decay) and p.requires_grad],
                "weight_decay": args.weight_decay
            },
            {
                "params": [p for n, p in model.named_parameters() 
                          if any(nd in n for nd in no_decay) and p.requires_grad],
                "weight_decay": 0.0
            }
        ]
        
        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)
        
        scheduler = get_scheduler(
            name=args.lr_scheduler_type,
            optimizer=optimizer,
            num_warmup_steps=int(args.max_train_steps * args.warmup_proportion),
            num_training_steps=args.max_train_steps
        )
        
        # å†»ç»“LMå‚æ•°
        if args.freeze_lm:
            trainable_params = ["prompt_embeddings", "prompt_lstm", "prompt_linear",
                               "detector_head", "candidate_head", "gated_fusion"]
            for n, p in model.named_parameters():
                if not any(tp in n for tp in trainable_params):
                    p.requires_grad = False
                    if is_main_process(args):
                        logger.info(f"Freeze: {n}")
        
        # æ··åˆç²¾åº¦
        scaler = None
        if args.fp16:
            from torch.amp import autocast, GradScaler
            scaler = GradScaler('cuda')
    
    # ============ è®­ç»ƒ ============
    if args.do_train:
        # åˆå§‹åŒ–TensorBoardï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
        tb_writer = None
        if is_main_process(args):
            tb_log_dir = os.path.join(args.output_dir, "tensorboard")
            os.makedirs(tb_log_dir, exist_ok=True)
            tb_writer = SummaryWriter(log_dir=tb_log_dir)
            logger.info(f"TensorBoard log dir: {tb_log_dir}")
        
        if is_main_process(args):
            logger.info("***** Running training *****")
            logger.info(f"  Num examples = {len(train_dataset)}")
            logger.info(f"  Per-device batch size = {per_device_batch_size}")
            logger.info(f"  Num GPUs = {dist.get_world_size() if dist.is_initialized() else 1}")
            logger.info(f"  Gradient accumulation steps = {args.gradient_accumulation_steps}")
            total_batch = per_device_batch_size * args.gradient_accumulation_steps * (dist.get_world_size() if dist.is_initialized() else 1)
            logger.info(f"  Total effective batch size = {total_batch}")
            logger.info(f"  Num epochs = {args.num_train_epochs}")
            logger.info(f"  Num update steps = {args.max_train_steps}")
            logger.info(f"  DataLoader num_workers = {args.num_workers}")
            
            # DDPä¼˜åŒ–æç¤º
            if args.local_rank != -1 and args.gradient_accumulation_steps > 1:
                logger.info(f"  ğŸš€ DDP no_syncä¼˜åŒ–å·²å¯ç”¨ï¼šæ¯{args.gradient_accumulation_steps}æ­¥é€šä¿¡1æ¬¡")
            
            # ä¿å­˜è®­ç»ƒå‚æ•°
            torch.save(args, os.path.join(args.output_dir, "train_args.bin"))
        
        global_step = 0
        best_result = []
        wrap = False
                # Early Stopping ç›¸å…³
        best_metric = 0.0
        patience_counter = 0
                # ç”¨äºç´¯ç§¯lossåˆ†é¡¹ä»¥ä¾¿è®°å½•å¹³å‡å€¼
        accumulated_losses = {
            'total': 0.0,
            'correction': 0.0,
            'detection': 0.0,
            'rank': 0.0
        }
        
        progress_bar = tqdm(range(args.max_train_steps), disable=not is_main_process(args))
        
        for epoch in range(int(args.num_train_epochs)):
            if wrap:
                break
            
            # DDPæ¯ä¸ªepoch shuffle
            if args.local_rank != -1:
                train_sampler.set_epoch(epoch)
            
            train_loss = 0
            train_steps = 0
            
            for step, batch in enumerate(train_dataloader):
                model.train()
                
                # è§£åŒ…batchï¼ˆå…¼å®¹TensorDatasetå…ƒç»„å’Œè‡ªå®šä¹‰Datasetå­—å…¸ä¸¤ç§æ ¼å¼ï¼‰
                if isinstance(batch, dict):
                    # è‡ªå®šä¹‰Datasetè¿”å›å­—å…¸
                    src_ids = batch['input_ids'].to(device)
                    attention_mask = batch['attention_mask'].to(device)
                    trg_ids = batch['labels'].to(device)
                    trg_ref_ids = batch['trg_ref_ids'].to(device)
                    block_flag = batch['block_flag'].to(device)
                    error_labels = batch['error_labels'].to(device)
                    candidate_ids = batch['candidate_ids'].to(device)
                else:
                    # TensorDatasetè¿”å›å…ƒç»„
                    src_ids, attention_mask, trg_ids, trg_ref_ids, block_flag, error_labels, candidate_ids = \
                        [t.to(device) for t in batch]
                
                # Masked-FT
                if args.mft:
                    src_ids = dynamic_mask_token(
                        src_ids, trg_ref_ids, tokenizer, device,
                        args.mask_mode, args.mask_rate
                    )
                
                # å¤„ç†labelsï¼ˆ-100 ignoreï¼‰
                labels = trg_ids.clone()
                labels[src_ids == trg_ids] = -100
                
                # å‰å‘ä¼ æ’­
                if args.fp16:
                    from torch.amp import autocast
                    with autocast('cuda'):
                        outputs = model(
                            input_ids=src_ids,
                            attention_mask=attention_mask,
                            prompt_mask=block_flag,
                            labels=labels,
                            candidate_ids=candidate_ids,
                            error_labels=error_labels,
                            apply_prompt=args.apply_prompt
                        )
                else:
                    outputs = model(
                        input_ids=src_ids,
                        attention_mask=attention_mask,
                        prompt_mask=block_flag,
                        labels=labels,
                        candidate_ids=candidate_ids,
                        error_labels=error_labels,
                        apply_prompt=args.apply_prompt
                    )
                
                loss = outputs['loss']
                
                # ç´¯ç§¯lossåˆ†é¡¹ç”¨äºTensorBoardè®°å½•
                accumulated_losses['total'] += loss.item()
                if outputs.get('correction_loss') is not None:
                    accumulated_losses['correction'] += outputs['correction_loss'].item()
                if outputs.get('detection_loss') is not None:
                    accumulated_losses['detection'] += outputs['detection_loss'].item()
                if outputs.get('rank_loss') is not None:
                    accumulated_losses['rank'] += outputs['rank_loss'].item()
                
                if args.gradient_accumulation_steps > 1:
                    loss = loss / args.gradient_accumulation_steps
                
                # åå‘ä¼ æ’­ï¼ˆä½¿ç”¨no_syncä¼˜åŒ–DDPé€šä¿¡ï¼‰
                # åœ¨æ¢¯åº¦ç´¯ç§¯çš„ä¸­é—´æ­¥éª¤è·³è¿‡AllReduceï¼Œåªåœ¨æœ€åä¸€æ­¥é€šä¿¡
                is_accumulation_step = (step + 1) % args.gradient_accumulation_steps != 0
                is_last_step = step == len(train_dataloader) - 1
                
                # å†³å®šæ˜¯å¦ä½¿ç”¨no_syncä¸Šä¸‹æ–‡
                if args.local_rank != -1 and is_accumulation_step and not is_last_step:
                    # DDPæ¨¡å¼ä¸‹çš„ä¸­é—´ç´¯ç§¯æ­¥éª¤ï¼šè·³è¿‡æ¢¯åº¦åŒæ­¥
                    sync_context = model.no_sync()
                else:
                    # éDDPæ¨¡å¼æˆ–éœ€è¦åŒæ­¥çš„æ­¥éª¤
                    sync_context = torch.enable_grad()
                
                with sync_context:
                    if args.fp16:
                        scaler.scale(loss).backward()
                    else:
                        loss.backward()
                
                train_loss += loss.item()
                train_steps += 1
                
                # æ¢¯åº¦æ›´æ–°
                if (step + 1) % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:
                    if args.fp16:
                        scaler.unscale_(optimizer)
                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                        optimizer.step()
                    
                    optimizer.zero_grad()
                    scheduler.step()
                    global_step += 1
                    progress_bar.update(1)
                
                # æ—¥å¿—è®°å½•
                if global_step % args.logging_steps == 0 and is_main_process(args):
                    avg_loss = train_loss / train_steps
                    logger.info(f"Step {global_step}, Loss: {avg_loss:.4f}")
                    
                    # TensorBoardè®°å½•è®­ç»ƒæŒ‡æ ‡
                    if tb_writer is not None:
                        # è®°å½•lossåˆ†é¡¹ï¼ˆå–å¹³å‡ï¼‰
                        log_steps = args.logging_steps
                        tb_writer.add_scalar('train/loss_total', accumulated_losses['total'] / log_steps, global_step)
                        if accumulated_losses['correction'] > 0:
                            tb_writer.add_scalar('train/loss_correction', accumulated_losses['correction'] / log_steps, global_step)
                        if accumulated_losses['detection'] > 0:
                            tb_writer.add_scalar('train/loss_detection', accumulated_losses['detection'] / log_steps, global_step)
                        if accumulated_losses['rank'] > 0:
                            tb_writer.add_scalar('train/loss_rank', accumulated_losses['rank'] / log_steps, global_step)
                        
                        # è®°å½•å­¦ä¹ ç‡
                        current_lr = scheduler.get_last_lr()[0]
                        tb_writer.add_scalar('train/learning_rate', current_lr, global_step)
                        
                        # é‡ç½®ç´¯ç§¯
                        accumulated_losses = {k: 0.0 for k in accumulated_losses}
                
                # éªŒè¯å’Œä¿å­˜
                # ä½¿ç”¨eval_stepsæ§åˆ¶è¯„ä¼°é¢‘ç‡ï¼Œé»˜è®¤ä¸save_stepsç›¸åŒ
                eval_step_interval = args.eval_steps if args.eval_steps else args.save_steps
                if args.do_eval and global_step % eval_step_interval == 0 and is_main_process(args):
                    eval_result = evaluate(
                        model, eval_dataloader, tokenizer, device, args, dgca_config
                    )
                    
                    logger.info(f"***** Eval results at step {global_step} *****")
                    for key, value in eval_result.items():
                        logger.info(f"  {key} = {value:.4f}")
                    
                    # TensorBoardè®°å½•éªŒè¯æŒ‡æ ‡
                    if tb_writer is not None:
                        tb_writer.add_scalar('eval/loss', eval_result['loss'], global_step)
                        tb_writer.add_scalar('eval/precision', eval_result['precision'], global_step)
                        tb_writer.add_scalar('eval/recall', eval_result['recall'], global_step)
                        tb_writer.add_scalar('eval/f1', eval_result['f1'], global_step)
                        tb_writer.add_scalar('eval/f2', eval_result['f2'], global_step)
                        tb_writer.add_scalar('eval/fpr', eval_result['fpr'], global_step)
                        tb_writer.add_scalar('eval/wpr', eval_result['wpr'], global_step)
                    
                    # ä¿å­˜æ¨¡å‹
                    model_to_save = model.module if hasattr(model, "module") else model
                    output_file = os.path.join(
                        args.output_dir,
                        f"step-{global_step}_f1-{eval_result['f1']:.2f}.bin"
                    )
                    torch.save(model_to_save.state_dict(), output_file)
                    
                    best_result.append((eval_result['f1'], output_file))
                    best_result.sort(key=lambda x: x[0], reverse=True)
                    
                    # åªä¿ç•™å‰3ä¸ªæœ€ä½³æ¨¡å‹
                    if len(best_result) > 3:
                        _, model_to_remove = best_result.pop()
                        if os.path.exists(model_to_remove):
                            os.remove(model_to_remove)
                    
                    # ä¿å­˜è¯„ä¼°ç»“æœ
                    with open(os.path.join(args.output_dir, "eval_results.txt"), "a") as f:
                        f.write(f"Step {global_step}: P={eval_result['precision']:.2f}, "
                               f"R={eval_result['recall']:.2f}, F1={eval_result['f1']:.2f}, "
                               f"F2={eval_result['f2']:.2f}, FPR={eval_result['fpr']:.2f}\n")
                    
                    # Early Stopping åˆ¤æ–­
                    if args.early_stopping_patience is not None:
                        current_metric = eval_result[args.early_stopping_metric]
                        if current_metric > best_metric:
                            best_metric = current_metric
                            patience_counter = 0
                            logger.info(f"New best {args.early_stopping_metric}: {best_metric:.4f}")
                        else:
                            patience_counter += 1
                            logger.info(f"Early stopping patience: {patience_counter}/{args.early_stopping_patience}")
                            
                            if patience_counter >= args.early_stopping_patience:
                                logger.info(f"Early stopping triggered! Best {args.early_stopping_metric}: {best_metric:.4f}")
                                wrap = True
                                break
                
                if global_step >= args.max_train_steps:
                    wrap = True
                    break
        
        # è®­ç»ƒç»“æŸï¼Œå…³é—­TensorBoard writer
        if tb_writer is not None:
            tb_writer.close()
            logger.info("TensorBoard writer closed")
    
    # ============ æµ‹è¯• ============
    if args.do_test:
        # ä¼˜å…ˆä½¿ç”¨é¢„å¤„ç†æ•°æ®
        if args.preprocessed_test:
            if is_main_process(args):
                logger.info(f"Loading preprocessed test data from {args.preprocessed_test}")
            
            # DDPæ¨¡å¼ä¸‹ï¼Œè®©rank 0å…ˆåŠ è½½ï¼Œå…¶ä»–rankç­‰å¾…ï¼Œé¿å…I/Oç«äº‰
            if args.local_rank != -1:
                if args.local_rank == 0:
                    test_dataset = PreprocessedDataset(args.preprocessed_test)
                dist.barrier()
                if args.local_rank != 0:
                    test_dataset = PreprocessedDataset(args.preprocessed_test)
            else:
                test_dataset = PreprocessedDataset(args.preprocessed_test)
        else:
            test_examples = processor.get_test_examples(args.data_dir, args.test_on)
            test_features = convert_examples_to_dgca_features(
                test_examples,
                args.max_seq_length,
                tokenizer,
                confusion_set,
                args.prompt_length,
                anchor=anchor
            )
            test_dataset = create_dgca_dataset(test_features)
        
        test_sampler = SequentialSampler(test_dataset)
        test_dataloader = DataLoader(
            test_dataset,
            sampler=test_sampler,
            batch_size=args.eval_batch_size,
            num_workers=args.num_workers,
            pin_memory=True
        )
        
        # å¦‚æœæ²¡æœ‰è®­ç»ƒï¼Œéœ€è¦é‡æ–°åŠ è½½æ¨¡å‹
        if not args.do_train and args.load_state_dict:
            bert_model = BertForMaskedLM.from_pretrained(
                args.load_model_path,
                return_dict=True,
                cache_dir=args.cache_dir
            )
            model = DGCAReLMWrapper(
                bert_model=bert_model,
                confusion_set=confusion_set,
                config=dgca_config,
                prompt_length=args.prompt_length
            )
            model.load_state_dict(torch.load(args.load_state_dict, map_location=device))
            model.to(device)
        
        if is_main_process(args):
            logger.info("***** Running test *****")
            logger.info(f"  Num examples = {len(test_examples)}")
        
        test_result = evaluate(
            model, test_dataloader, tokenizer, device, args, dgca_config,
            save_predictions=True,
            output_dir=args.output_dir
        )
        
        if is_main_process(args):
            logger.info("***** Test results *****")
            for key, value in test_result.items():
                logger.info(f"  {key} = {value:.4f}")
    
    # æ¸…ç†DDP
    cleanup_ddp()


def evaluate(model, dataloader, tokenizer, device, args, config,
             save_predictions=False, output_dir=None):
    """è¯„ä¼°å‡½æ•°"""
    model.eval()
    
    all_inputs, all_labels, all_predictions = [], [], []
    eval_loss = 0
    eval_steps = 0
    
    for batch in tqdm(dataloader, desc="Evaluating"):
        # å…¼å®¹TensorDatasetå…ƒç»„å’Œè‡ªå®šä¹‰Datasetå­—å…¸ä¸¤ç§æ ¼å¼
        if isinstance(batch, dict):
            src_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            trg_ids = batch['labels'].to(device)
            trg_ref_ids = batch['trg_ref_ids'].to(device)
            block_flag = batch['block_flag'].to(device)
            error_labels = batch['error_labels'].to(device)
            candidate_ids = batch['candidate_ids'].to(device)
        else:
            src_ids, attention_mask, trg_ids, trg_ref_ids, block_flag, error_labels, candidate_ids = \
                [t.to(device) for t in batch]
        
        labels = trg_ids.clone()
        labels[src_ids == trg_ids] = -100
        
        with torch.no_grad():
            outputs = model(
                input_ids=src_ids,
                attention_mask=attention_mask,
                prompt_mask=block_flag,
                labels=labels,
                candidate_ids=candidate_ids,
                error_labels=error_labels,
                apply_prompt=args.apply_prompt
            )
            
            logits = outputs['logits']
            if outputs['loss'] is not None:
                eval_loss += outputs['loss'].item()
        
        # è§£ç é¢„æµ‹ç»“æœ
        _, prd_ids = torch.max(logits, dim=-1)
        prd_ids = prd_ids.masked_fill(attention_mask == 0, 0)
        
        src_ids_list = src_ids.tolist()
        trg_ids_list = trg_ids.tolist()
        prd_ids_list = prd_ids.tolist()
        
        for s, t, p in zip(src_ids_list, trg_ids_list, prd_ids_list):
            mapped_src, mapped_trg, mapped_prd = [], [], []
            flag = False
            
            for st, tt, pt in zip(s, t, p):
                if st == tokenizer.sep_token_id:
                    flag = True
                
                if not flag:
                    mapped_src.append(st)
                else:
                    mapped_trg.append(tt)
                    if st == tokenizer.mask_token_id:
                        mapped_prd.append(pt)
                    else:
                        mapped_prd.append(st)
            
            # å¤„ç†anchor
            if args.anchor is not None:
                anchor_length = len(args.anchor) + 1
                del mapped_trg[:anchor_length]
                del mapped_prd[:anchor_length]
            
            def decode(ids):
                return tokenizer.convert_ids_to_tokens(ids, skip_special_tokens=True)
            
            all_inputs.append(decode(mapped_src))
            all_labels.append(decode(mapped_trg))
            all_predictions.append(decode(mapped_prd))
        
        eval_steps += 1
    
    # è®¡ç®—æŒ‡æ ‡
    p, r, f1, fpr, wpr, tp_sents, fp_sents, fn_sents, wp_sents = \
        Metrics.csc_compute(all_inputs, all_labels, all_predictions)
    
    # è®¡ç®—F2ï¼ˆÎ²=2ï¼Œæ›´åå‘å¬å›ï¼‰
    beta = 2
    f2 = (1 + beta**2) * (p * r) / (beta**2 * p + r + 1e-12)
    
    # ä¿å­˜é¢„æµ‹ç»“æœ
    if save_predictions and output_dir:
        with open(os.path.join(output_dir, "sents.tp"), "w") as f:
            for line in tp_sents:
                f.write(line + "\n")
        
        with open(os.path.join(output_dir, "sents.fp"), "w") as f:
            for line in fp_sents:
                f.write(line + "\n")
        
        with open(os.path.join(output_dir, "sents.fn"), "w") as f:
            for line in fn_sents:
                f.write(line + "\n")
        
        with open(os.path.join(output_dir, "sents.wp"), "w") as f:
            for line in wp_sents:
                f.write(line + "\n")
    
    return {
        'loss': eval_loss / max(eval_steps, 1),
        'precision': p * 100,
        'recall': r * 100,
        'f1': f1 * 100,
        'f2': f2 * 100,
        'fpr': fpr * 100,
        'wpr': wpr * 100
    }


if __name__ == "__main__":
    main()
